{
 "cells": [
  {
   "source": [
    "# Cyberbullying model using LSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "source": [
    "## Preprocessing the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_bullying</th>\n",
       "      <th>text_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah I got 2 backups for all that. I just hate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I hate using my BB  but love my iPhone. Haven'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_bullying                                       text_message\n",
       "0               0  yeah I got 2 backups for all that. I just hate...\n",
       "1               0  I hate using my BB  but love my iPhone. Haven'...\n",
       "2               1                             Get fucking real dude.\n",
       "3               1   She is as dirty as they come  and that crook ...\n",
       "4               1   why did you fuck it up. I could do it all day..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"anti-bully-data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df['text_message']\n",
    "y = df['label_bullying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       yeah I got 2 backups for all that. I just hate...\n",
       "1       I hate using my BB  but love my iPhone. Haven'...\n",
       "2                                  Get fucking real dude.\n",
       "3        She is as dirty as they come  and that crook ...\n",
       "4        why did you fuck it up. I could do it all day...\n",
       "5        Dude they dont finish enclosing the fucking s...\n",
       "6        WTF are you talking about Men? No men thats n...\n",
       "7       Ill save you the trouble sister. Here comes a ...\n",
       "8        Im dead serious.Real athletes never cheat don...\n",
       "9           wow lol sounds like a lot of piss then hehehe\n",
       "10      not a damn thang..the typical rap beef. one pe...\n",
       "11      ...go absolutely insane.hate to be the bearer ...\n",
       "12      well damn!! where have you been when i have ne...\n",
       "13      watching without a trace too...hate when i mis...\n",
       "14      which they do most of the time:-P I don't hate...\n",
       "15      Lmao  im watching the same thing ahaha. The ga...\n",
       "16      LOL  no he said  What do you call a jail cell ...\n",
       "17      truth on both counts that guy is an ass  and t...\n",
       "18                                      Shakespeare nerd!\n",
       "19                            you are SUCH a fucking dork\n",
       "20                                 Heh. Fuck 'em WHERE?!?\n",
       "21                     damn it i totally forgot that one!\n",
       "22                        haha fuck i wish i was there :(\n",
       "23      paranoid is wack as fuck...the best song on 80...\n",
       "24           wow  damn I would have been pissed @ that...\n",
       "25          nigga u geigh lmao! fuck yo finals beeeeeitch\n",
       "26      in London  I hate thee :( ENJOY YOURSELVES!!! ...\n",
       "27      lollipop lolipop...oh loli loli loli....duh du...\n",
       "28                                          that sucks :(\n",
       "29      read that this morning. my fav is how they jus...\n",
       "                              ...                        \n",
       "8787    who'd you piss off? oh...wait...that's a ridic...\n",
       "8788    man  they photoshopped the fuck out of summer ...\n",
       "8789    I'm not feeling emo. I'm more in a relentless-...\n",
       "8790    I've tried giving you treats  wet food  milk. ...\n",
       "8791    damn. i tried to talk peeps into venturing els...\n",
       "8792    I agree. There are a lot of pregnant women and...\n",
       "8793    I hadn't really known a lot about her until I ...\n",
       "8794                                   Thanks coke whore!\n",
       "8795                   Because you're a coke whore maybe?\n",
       "8796    damn... U should just stay home and call it a ...\n",
       "8797                 oh my god.... XD sucks for you Talei\n",
       "8798    yeah yeah. I've seen your dart board with thei...\n",
       "8799    It sucks but at least my car wasn't stolen. ri...\n",
       "8800                                It may even suck ass.\n",
       "8801                                             Ah. Gay!\n",
       "8802    I hate you so much I just want to smash your f...\n",
       "8803    Yes you are a loser... I could have told you t...\n",
       "8804    just because of a dang song. Over it now but d...\n",
       "8805    I dunno  I hate it though.  Someone at work ga...\n",
       "8806    I've made myself achingly sore playing mario a...\n",
       "8807    I like shopping  I hate lines. My therapist as...\n",
       "8808    been there done that  it really sucks. One of ...\n",
       "8809    Dude  I've given up. After a girl who didn't w...\n",
       "8810    he doesnt know i know hes gay or bi or w.e. He...\n",
       "8811    its not just that Corinne is a bitch  she's pr...\n",
       "8812    no caffiene would kick my ass too - I'm addict...\n",
       "8813    Now I'm hungry.  Damn you people and your midn...\n",
       "8814    i've taken one also. people just piss me off m...\n",
       "8815    That  too! Or even being able to park pulling ...\n",
       "8816                          . . . HE'S FUCKING HIMSELF!\n",
       "Name: text_message, Length: 8817, dtype: object"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag) :\n",
    "    if tag.startswith('J') :\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V') :\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N') :\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R') :\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(review) :\n",
    "    global max_len\n",
    "    words = word_tokenize(review)\n",
    "    output_words = []\n",
    "    for word in words :\n",
    "        if word.lower() not in stop_words :\n",
    "            pos = pos_tag([word])\n",
    "            clean_word = lemmatizer.lemmatize(word,pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    max_len = max(max_len, len(output_words))\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah get 2 backup hate happen strugglin week ... handle tho\n",
      "yeah get 2 backup hate happen strugglin week ... handle tho\n"
     ]
    }
   ],
   "source": [
    "print(messages[0])\n",
    "messages = [clean_text(message) for message in messages]\n",
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as file:\n",
    "        word_to_vec_map = {}\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        index = 0\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            word_to_index[curr_word] = index\n",
    "            index_to_word[index] = curr_word\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            index += 1\n",
    "    return word_to_index, index_to_word, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = len(X)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        j = 0\n",
    "        for word in sentence_words:\n",
    "            if word in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[word]\n",
    "            j += 1\n",
    "    return X_indices"
   ]
  },
  {
   "source": [
    "## The LSTM model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPModel(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    embeddings = pretrained_embedding_layer(word_to_vec_map, word_to_index)(sentence_indices)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(1)(X)\n",
    "    X = Activation('sigmoid')(X)\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_23 (Embedding)     (None, 30, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 30, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,411\n",
      "Trainable params: 223,361\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NLPModel((max_len,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(messages, y, random_state = 0, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7935/7935 [==============================] - 12s 1ms/step - loss: 0.6068 - accuracy: 0.7156\n",
      "Epoch 2/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.6007 - accuracy: 0.7166\n",
      "Epoch 3/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5995 - accuracy: 0.7166\n",
      "Epoch 4/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.5997 - accuracy: 0.7166\n",
      "Epoch 5/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5990 - accuracy: 0.7166\n",
      "Epoch 6/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.5992 - accuracy: 0.7166\n",
      "Epoch 7/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.5983 - accuracy: 0.7166\n",
      "Epoch 8/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5982 - accuracy: 0.7166\n",
      "Epoch 9/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5972 - accuracy: 0.7159\n",
      "Epoch 10/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5880 - accuracy: 0.7147\n",
      "Epoch 11/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5811 - accuracy: 0.7162\n",
      "Epoch 12/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5780 - accuracy: 0.7152 0s - loss: 0.5761 - accuracy: 0.\n",
      "Epoch 13/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5694 - accuracy: 0.7206 0s - loss: 0.5\n",
      "Epoch 14/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5638 - accuracy: 0.7264\n",
      "Epoch 15/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5590 - accuracy: 0.7288\n",
      "Epoch 16/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.5518 - accuracy: 0.7336\n",
      "Epoch 17/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.5423 - accuracy: 0.7454\n",
      "Epoch 18/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.5313 - accuracy: 0.7544\n",
      "Epoch 19/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.5195 - accuracy: 0.7641\n",
      "Epoch 20/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.5063 - accuracy: 0.7762\n",
      "Epoch 21/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.4914 - accuracy: 0.7905\n",
      "Epoch 22/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.4705 - accuracy: 0.8020\n",
      "Epoch 23/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.4532 - accuracy: 0.8118\n",
      "Epoch 24/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.4304 - accuracy: 0.8257\n",
      "Epoch 25/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.4198 - accuracy: 0.8348\n",
      "Epoch 26/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.3965 - accuracy: 0.8470\n",
      "Epoch 27/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.3775 - accuracy: 0.8534\n",
      "Epoch 28/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.3661 - accuracy: 0.8611\n",
      "Epoch 29/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.3510 - accuracy: 0.8691\n",
      "Epoch 30/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.3474 - accuracy: 0.8694\n",
      "Epoch 31/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.3245 - accuracy: 0.8822\n",
      "Epoch 32/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.3124 - accuracy: 0.8830\n",
      "Epoch 33/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2917 - accuracy: 0.8934\n",
      "Epoch 34/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2946 - accuracy: 0.8917\n",
      "Epoch 35/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2911 - accuracy: 0.8938\n",
      "Epoch 36/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.2725 - accuracy: 0.8994\n",
      "Epoch 37/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.2697 - accuracy: 0.9020\n",
      "Epoch 38/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.2564 - accuracy: 0.9046\n",
      "Epoch 39/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.2440 - accuracy: 0.9085\n",
      "Epoch 40/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2406 - accuracy: 0.9109\n",
      "Epoch 41/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2332 - accuracy: 0.9109\n",
      "Epoch 42/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2212 - accuracy: 0.9129\n",
      "Epoch 43/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2119 - accuracy: 0.9168\n",
      "Epoch 44/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.2109 - accuracy: 0.9162\n",
      "Epoch 45/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.2016 - accuracy: 0.9149\n",
      "Epoch 46/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.1933 - accuracy: 0.9161\n",
      "Epoch 47/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.1764 - accuracy: 0.9234\n",
      "Epoch 48/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.1795 - accuracy: 0.9254\n",
      "Epoch 49/50\n",
      "7935/7935 [==============================] - 10s 1ms/step - loss: 0.1732 - accuracy: 0.9299\n",
      "Epoch 50/50\n",
      "7935/7935 [==============================] - 11s 1ms/step - loss: 0.1537 - accuracy: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1db47d04ef0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train, epochs = 50, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882/882 [==============================] - 1s 582us/step\n",
      "Test accuracy =  0.6734693646430969\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test_indices, Y_test)\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "source": [
    "## Accuracy of LSTM: 67.25%"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "source": [
    "## Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suck']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"suck it\"\n",
    "text = [clean_text(text)]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sentences_to_indices(text, word_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23695.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48035333"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text)[0][0]"
   ]
  },
  {
   "source": [
    "## Extras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_to_index, open('word_to_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c4d2db86565210e44e1312e025fd2a01c5965d45dad733b18a9ee28031514e1f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}